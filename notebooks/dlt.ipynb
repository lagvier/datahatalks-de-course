{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bc1fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a72c6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"vendor_name\": \"VTS\",\n",
    "        \"record_hash\": \"b00361a396177a9cb410ff61f20015ad\",\n",
    "        \"time\": {\n",
    "            \"pickup\": \"2009-06-14 23:23:00\",\n",
    "            \"dropoff\": \"2009-06-14 23:48:00\"\n",
    "        },\n",
    "        \"Trip_Distance\": 17.52,\n",
    "        \"coordinates\": {\n",
    "            \"start\": {\n",
    "                \"lon\": -73.787442,\n",
    "                \"lat\": 40.641525\n",
    "            },\n",
    "            \"end\": {\n",
    "                \"lon\": -73.980072,\n",
    "                \"lat\": 40.742963\n",
    "            }\n",
    "        },\n",
    "        \"Rate_Code\": None,\n",
    "        \"store_and_forward\": None,\n",
    "        \"Payment\": {\n",
    "            \"type\": \"Credit\",\n",
    "            \"amt\": 20.5,\n",
    "            \"surcharge\": 0,\n",
    "            \"mta_tax\": None,\n",
    "            \"tip\": 9,\n",
    "            \"tolls\": 4.15,\n",
    "            \"status\": \"booked\"\n",
    "        },\n",
    "        \"Passenger_Count\": 2,\n",
    "        \"passengers\": [\n",
    "            {\"name\": \"John\", \"rating\": 4.9},\n",
    "            {\"name\": \"Jack\", \"rating\": 3.9}\n",
    "        ],\n",
    "        \"Stops\": [\n",
    "            {\"lon\": -73.6, \"lat\": 40.6},\n",
    "            {\"lon\": -73.5, \"lat\": 40.5}\n",
    "        ]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41058cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline taxi_data load step completed in 0.54 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset taxi_rides\n",
      "The duckdb destination used duckdb:///C:\\Personal\\Training\\dataengineering\\notebooks\\taxi_data.duckdb location to store data\n",
      "Load package 1707413142.410891 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline to database\n",
    "pipeline = dlt.pipeline(pipeline_name=\"taxi_data\",\n",
    "                        destination='duckdb', \n",
    "                        dataset_name='taxi_rides')\n",
    "# run pipeline\n",
    "info = pipeline.run(data, \n",
    "                    table_name=\"users\", \n",
    "                    write_disposition=\"replace\")\n",
    "\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14359891",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [\n",
    "    {\n",
    "        \"vendor_name\": \"VTS\",\n",
    "\t\t\"record_hash\": \"b00361a396177a9cb410ff61f20015ad\",\n",
    "        \"time\": {\n",
    "            \"pickup\": \"2009-06-14 23:23:00\",\n",
    "            \"dropoff\": \"2009-06-14 23:48:00\"\n",
    "        },\n",
    "        \"Trip_Distance\": 17.52,\n",
    "        \"coordinates\": {\n",
    "            \"start\": {\n",
    "                \"lon\": -73.787442,\n",
    "                \"lat\": 40.641525\n",
    "            },\n",
    "            \"end\": {\n",
    "                \"lon\": -73.980072,\n",
    "                \"lat\": 40.742963\n",
    "            }\n",
    "        },\n",
    "        \"Rate_Code\": None,\n",
    "        \"store_and_forward\": None,\n",
    "        \"Payment\": {\n",
    "            \"type\": \"Credit\",\n",
    "            \"amt\": 20.5,\n",
    "            \"surcharge\": 0,\n",
    "            \"mta_tax\": None,\n",
    "            \"tip\": 9,\n",
    "            \"tolls\": 4.15,\n",
    "\t\t\t\"status\": \"cancelled\"\n",
    "        },\n",
    "        \"Passenger_Count\": 2,\n",
    "        \"passengers\": [\n",
    "            {\"name\": \"John\", \"rating\": 4.4},\n",
    "            {\"name\": \"Jack\", \"rating\": 3.6}\n",
    "        ],\n",
    "        \"Stops\": [\n",
    "            {\"lon\": -73.6, \"lat\": 40.6},\n",
    "            {\"lon\": -73.5, \"lat\": 40.5}\n",
    "        ]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3c0a2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline dlt_ipykernel_launcher load step completed in 1.05 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset taxi_rides\n",
      "The duckdb destination used duckdb:///C:\\Personal\\Training\\dataengineering\\notebooks\\dlt_ipykernel_launcher.duckdb location to store data\n",
      "Load package 1707416754.972069 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# define the connection to load to. \n",
    "# We now use duckdb, but you can switch to Bigquery later\n",
    "pipeline = dlt.pipeline(destination='duckdb', dataset_name='taxi_rides')\n",
    "\n",
    "# run the pipeline with default settings, and capture the outcome\n",
    "info = pipeline.run(data1, \n",
    "                    table_name=\"users\", \n",
    "                    write_disposition=\"merge\", \n",
    "                    primary_key=\"record_hash\")\n",
    "\n",
    "# show the outcome\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6156a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method run in module dlt.pipeline.pipeline:\n",
      "\n",
      "run(data: Any = None, *, destination: Union[str, ForwardRef('Destination'), Callable[..., ForwardRef('Destination')], NoneType] = None, staging: Union[str, ForwardRef('Destination'), Callable[..., ForwardRef('Destination')], NoneType] = None, dataset_name: str = None, credentials: Any = None, table_name: str = None, write_disposition: Literal['skip', 'append', 'replace', 'merge'] = None, columns: Union[Dict[str, dlt.common.schema.TColumnSchema], Sequence[dlt.common.schema.TColumnSchema], pydantic.main.BaseModel, Type[pydantic.main.BaseModel]] = None, primary_key: Union[str, Sequence[str]] = None, schema: dlt.common.schema.schema.Schema = None, loader_file_format: Literal['jsonl', 'puae-jsonl', 'insert_values', 'sql', 'parquet', 'reference', 'arrow'] = None, schema_contract: Union[Literal['evolve', 'discard_value', 'freeze', 'discard_row'], dlt.common.schema.TSchemaContractDict] = None) -> dlt.common.pipeline.LoadInfo method of dlt.pipeline.pipeline.Pipeline instance\n",
      "    Loads the data from `data` argument into the destination specified in `destination` and dataset specified in `dataset_name`.\n",
      "    \n",
      "    #### Note:\n",
      "    This method will `extract` the data from the `data` argument, infer the schema, `normalize` the data into a load package (ie. jsonl or PARQUET files representing tables) and then `load` such packages into the `destination`.\n",
      "    \n",
      "    The data may be supplied in several forms:\n",
      "    * a `list` or `Iterable` of any JSON-serializable objects ie. `dlt.run([1, 2, 3], table_name=\"numbers\")`\n",
      "    * any `Iterator` or a function that yield (`Generator`) ie. `dlt.run(range(1, 10), table_name=\"range\")`\n",
      "    * a function or a list of functions decorated with @dlt.resource ie. `dlt.run([chess_players(title=\"GM\"), chess_games()])`\n",
      "    * a function or a list of functions decorated with @dlt.source.\n",
      "    \n",
      "    Please note that `dlt` deals with `bytes`, `datetime`, `decimal` and `uuid` objects so you are free to load documents containing ie. binary data or dates.\n",
      "    \n",
      "    #### Execution:\n",
      "    The `run` method will first use `sync_destination` method to synchronize pipeline state and schemas with the destination. You can disable this behavior with `restore_from_destination` configuration option.\n",
      "    Next it will make sure that data from the previous is fully processed. If not, `run` method normalizes, loads pending data items and **exits**\n",
      "    If there was no pending data, new data from `data` argument is extracted, normalized and loaded.\n",
      "    \n",
      "    #### Args:\n",
      "        data (Any): Data to be loaded to destination\n",
      "    \n",
      "        destination (str | DestinationReference, optional): A name of the destination to which dlt will load the data, or a destination module imported from `dlt.destination`.\n",
      "        If not provided, the value passed to `dlt.pipeline` will be used.\n",
      "    \n",
      "        dataset_name (str, optional):A name of the dataset to which the data will be loaded. A dataset is a logical group of tables ie. `schema` in relational databases or folder grouping many files.\n",
      "        If not provided, the value passed to `dlt.pipeline` will be used. If not provided at all then defaults to the `pipeline_name`\n",
      "    \n",
      "    \n",
      "        credentials (Any, optional): Credentials for the `destination` ie. database connection string or a dictionary with google cloud credentials.\n",
      "        In most cases should be set to None, which lets `dlt` to use `secrets.toml` or environment variables to infer right credentials values.\n",
      "    \n",
      "        table_name (str, optional): The name of the table to which the data should be loaded within the `dataset`. This argument is required for a `data` that is a list/Iterable or Iterator without `__name__` attribute.\n",
      "        The behavior of this argument depends on the type of the `data`:\n",
      "        * generator functions: the function name is used as table name, `table_name` overrides this default\n",
      "        * `@dlt.resource`: resource contains the full table schema and that includes the table name. `table_name` will override this property. Use with care!\n",
      "        * `@dlt.source`: source contains several resources each with a table schema. `table_name` will override all table names within the source and load the data into single table.\n",
      "    \n",
      "        write_disposition (Literal[\"skip\", \"append\", \"replace\", \"merge\"], optional): Controls how to write data to a table. `append` will always add new data at the end of the table. `replace` will replace existing data with new data. `skip` will prevent data from loading. \"merge\" will deduplicate and merge data based on \"primary_key\" and \"merge_key\" hints. Defaults to \"append\".\n",
      "        Please note that in case of `dlt.resource` the table schema value will be overwritten and in case of `dlt.source`, the values in all resources will be overwritten.\n",
      "    \n",
      "        columns (Sequence[TColumnSchema], optional): A list of column schemas. Typed dictionary describing column names, data types, write disposition and performance hints that gives you full control over the created table schema.\n",
      "    \n",
      "        primary_key (str | Sequence[str]): A column name or a list of column names that comprise a private key. Typically used with \"merge\" write disposition to deduplicate loaded data.\n",
      "    \n",
      "        schema (Schema, optional): An explicit `Schema` object in which all table schemas will be grouped. By default `dlt` takes the schema from the source (if passed in `data` argument) or creates a default one itself.\n",
      "    \n",
      "        loader_file_format (Literal[\"jsonl\", \"insert_values\", \"parquet\"], optional). The file format the loader will use to create the load package. Not all file_formats are compatible with all destinations. Defaults to the preferred file format of the selected destination.\n",
      "    \n",
      "        schema_contract (TSchemaContract, optional): On override for the schema contract settings, this will replace the schema contract settings for all tables in the schema. Defaults to None.\n",
      "    \n",
      "    Raises:\n",
      "        PipelineStepFailed when a problem happened during `extract`, `normalize` or `load` steps.\n",
      "    Returns:\n",
      "        LoadInfo: Information on loaded data including the list of package ids and failed job statuses. Please not that `dlt` will not raise if a single job terminally fails. Such information is provided via LoadInfo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pipeline.run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
